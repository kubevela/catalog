# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/serving.cue
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Model serving uses Seldon deployment to serve models.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: model-serving
spec:
  schematic:
    cue:
      template: |
        implMap: {
        	tensorflow: "TENSORFLOW_SERVER"
        	xgboost:    "XGBOOST_SERVER"
        	sklearn:    "SKLEARN_SERVER"
        	mlflow:     "MLFLOW_SERVER"
        	custom:     "CUSTOM_SERVER"
        }
        output: {
        	apiVersion: "machinelearning.seldon.io/v1alpha2"
        	kind:       "SeldonDeployment"
        	metadata: {
        		name:      context.name
        		namespace: context.namespace
        		annotations: {
        			if parameter.timeout != _|_ {
        				"seldon.io/rest-timeout": parameter.timeout
        				"seldon.io/grpc-timeout": parameter.timeout
        			}
        		}
        	}
        	spec: {
        		annotations: {
        			if parameter.customRouting != _|_ {
        				"seldon.io/ambassador-header":       parameter.customRouting.header
        				"seldon.io/ambassador-service-name": parameter.customRouting.serviceName
        			}
        		}
        		if parameter.protocol != _|_ {
        			protocol: parameter.protocol
        		}
        		predictors: [ for p in parameter.predictors {
        			{
        				name: p.name
        				if p.replicas != _|_ {
        					replicas: p.replicas
        				}
        				if p.traffic != _|_ {
        					traffic: p.traffic
        				}
        				graph: {
        					name:           p.graph.name
        					implementation: implMap[p.graph.implementation]
        					modelUri:       p.graph.modelUri
        				}
        				if p.resources != _|_ || p.autoscaler != _|_ {
        					componentSpecs: [{
        						if p.autoscaler != _|_ {
        							hpaSpec: {
        								minReplicas: p.autoscaler.minReplicas
        								maxReplicas: p.autoscaler.maxReplicas
        								metrics: [ for m in p.autoscaler.metrics {
        									{
        										resource: {
        											name:                     m.type
        											targetAverageUtilization: m.targetAverageUtilization
        										}
        										type: "Resource"
        									}
        								}]
        							}
        						}
        						spec: containers: [{
        							name: p.graph.name
        							if p.resources.cpu != _|_ {
        								resources: {
        									limits: cpu:   p.resources.cpu
        									requests: cpu: p.resources.cpu
        								}
        							}

        							if p.resources.memory != _|_ {
        								resources: {
        									limits: memory:   p.resources.memory
        									requests: memory: p.resources.memory
        								}
        							}

        							if p.resources.gpu != _|_ {
        								resources: {
        									limits: "nvidia.com/gpu":   p.resources.gpu
        									requests: "nvidia.com/gpu": p.resources.gpu
        								}
        							}
        						}]
        					}]
        				}
        			}
        		}]
        	}
        }
        parameter: {
        	// +usage=Specify the custom routing of the serving
        	customRouting?: {
        		// +usage=Request with specified header will be routed to the specified service
        		header: string
        		// +usage=The service name that will be routed to
        		serviceName: string
        	}
        	// +usage=Protocol of model serving, default to seldon
        	protocol?: "seldon" | "tensorflow" | "v2"
        	// +usage=If you model serving need long time to return, please set the timeout
        	timeout?: string
        	// +usage=The predictors of the serving
        	predictors: [...{
        		// +usage=Name of the predictor
        		name: string
        		// +usage=Replica of the predictor
        		replicas?: int
        		// +usage=If you want to split the traffic to different serving, please set the traffic here
        		traffic?: int
        		// +usage=The graph of the predictor
        		graph: {
        			// +usage=The name of the graph
        			name: string
        			// +usage=The model uri, you can use `pvc://pvc-name/path` or `s3://s3-name/path`, etc.
        			modelUri: string
        			// +usage=The implementation of the serving
        			implementation: "custom" | "tensorflow" | "sklearn" | "xgboost" | "mlflow"
        		}
        		// +usage=The resources of the serving
        		resources?: {
        			// +usage=Number of CPU units for the service, like `0.5` (0.5 CPU core), `1` (1 CPU core)
        			cpu?: string
        			// +usage=Specifies the attributes of the memory resource required for the container.
        			memory?: string
        			// +usage=Specifies the attributes of the gpu resource required for the container.
        			gpu?: string
        		}
        		// +usage=The autoscaler of the serving
        		autoscaler?: {
        			// +usage=The max replicas of this auto scaler
        			maxReplicas: int
        			// +usage=The min replicas of this auto scaler
        			minReplicas: int
        			// +usage=The metrics of this auto scaler
        			metrics: [...{
        				// +usage=The target average utilization of this auto scaler
        				targetAverageUtilization: int
        				// +usage=The type of this auto scaler
        				type: "cpu" | "memory"
        			}]
        		}
        	}]
        }
  status:
    customStatus: |-
      if context.output.status.state != _|_ {
      	message: context.output.status.state
      }
    healthPolicy: |-
      if context.output.status.state != _|_ {
      	isHealth: context.output.status.state == "Available"
      }
      if context.output.status.state == _|_ {
      	isHealth: false
      }
  workload:
    type: autodetects.core.oam.dev
