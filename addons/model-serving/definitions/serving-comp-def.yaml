# Code generated by KubeVela templates. DO NOT EDIT. Please edit the original cue file.
# Definition source cue file: vela-templates/definitions/internal/serving.cue
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Serving uses Seldon deployment to serve models
  name: serving
  namespace: vela-system
spec:
  schematic:
    cue:
      template: |
        implMap: {
        	tensorflow: "TENSORFLOW_SERVER"
        	xgboost:    "XGBOOST_SERVER"
        	sklearn:    "SKLEARN_SERVER"
        	mlflow:     "MLFLOW_SERVER"
        	custom:     "CUSTOM_SERVER"
        }
        output: {
        	apiVersion: "machinelearning.seldon.io/v1alpha2"
        	kind:       "SeldonDeployment"
        	metadata: {
        		name:      context.name
        		namespace: context.namespace
        	}
        	spec: {
        		if parameter.customRouting != _|_ {
        			annotations: {
        				"seldon.io/ambassador-header":       parameter.customRouting.header
        				"seldon.io/ambassador-service-name": parameter.customRouting.serviceName
        			}
        		}
        		predictors: [ for p in parameter.predictors {
        			{
        				name: p.name
        				if p.replicas != _|_ {
        					replicas: p.replicas
        				}
        				if p.traffic != _|_ {
        					traffic: p.traffic
        				}
        				graph: {
        					name:           p.graph.name
        					implementation: implMap[p.graph.implementation]
        					modelUri:       p.graph.modelUri
        				}
        				if p.resources != _|_ || p.autoscaler != _|_ {
        					componentSpecs: [{
        						if p.autoscaler != _|_ {
        							hpaSpec: {
        								minReplicas: p.autoscaler.minReplicas
        								maxReplicas: p.autoscaler.maxReplicas
        								metrics: [ for m in p.autoscaler.metrics {
        									{
        										resource: {
        											name:                     m.type
        											targetAverageUtilization: m.targetAverageUtilization
        										}
        										type: "Resource"
        									}
        								}]
        							}
        						}
        						spec: containers: [{
        							name: p.graph.name
        							if p.resources.cpu != _|_ {
        								resources: {
        									limits: cpu:   p.resources.cpu
        									requests: cpu: p.resources.cpu
        								}
        							}

        							if p.resources.memory != _|_ {
        								resources: {
        									limits: memory:   p.resources.memory
        									requests: memory: p.resources.memory
        								}
        							}

        							if p.resources.gpu != _|_ {
        								resources: {
        									limits: "nvidia.com/gpu":   p.resources.gpu
        									requests: "nvidia.com/gpu": p.resources.gpu
        								}
        							}
        						}]
        					}]
        				}
        			}
        		}]
        	}
        }
        parameter: {
        	protocol: *"seldon" | "tensorflow" | ""
        	customRouting?: {
        		header:      string
        		serviceName: string
        	}
        	predictors: [...{
        		name:      string
        		replicas?: int
        		traffic?:  int
        		graph: {
        			name:           string
        			modelUri:       string
        			implementation: "custom" | "tensorflow" | "sklearn" | "xgboost" | "mlflow"
        		}
        		resources?: {
        			// +usage=Number of CPU units for the service, like `0.5` (0.5 CPU core), `1` (1 CPU core)
        			cpu?: string
        			// +usage=Specifies the attributes of the memory resource required for the container.
        			memory?: string
        			// +usage=Specifies the attributes of the gpu resource required for the container.
        			gpu?: string
        		}
        		autoscaler?: {
        			maxReplicas: int
        			minReplicas: int
        			metrics: [...{
        				targetAverageUtilization: int
        				type:                     "cpu" | "memory"
        			}]
        		}
        	}]
        }
  status:
    customStatus: |-
      if context.output.status.state != _|_ {
      	message: context.output.status.state
      }
    healthPolicy: |-
      if context.output.status.state != _|_ {
      	isHealth: context.output.status.state == "Available"
      }
      if context.output.status.state == _|_ {
      	isHealth: false
      }
  workload:
    type: autodetects.core.oam.dev
